-------------------------------------------------------------------LOG ANALYSER APP----------------------------------------------------------------

OverView
    The Log Analyzer App is capable of processing, validating, and analyzing logs from various file formats such as JSON, CSV, plain text. It enables filtering, searching, and sorting logs while storing them in Elasticsearch and MongoDB for scalable and efficient querying.

Setup Instructions
    1.Node.js: Install Node.js (6.7 or later driver version expected).
    2.MongoDB: Ensure a running MongoDB instance (local or MongoDB Atlas).
    3.Elasticsearch: Install and run Elasticsearch (v7.x or higher).
    4.Postman: Recommended for API testing.

Installation
    1.Clone the repository: git clone https://github.com/mani-chauhan2004/log-analyzer-app.git
    2.Use terminal to navigate to the log-analyzer-app folder: cd log-analyzer-app
    3.Install the required dependencies(refer to package.json for more information): npm install
    4.Start the app: nodemon app.js --ignore uploads/


API Refrence 
    1. Base URLs
        Primary: http://localhost:5000/api/logs
        Secondary(In case port 5000 has busy status): http://localhost:3000/api

    2.Endpoints
        a. Upload Logs
            a. POST /upload
                -> Description: Upload one or multiple log files for processing.
                -> Headers: Content-Type: multipart/form-data
                -> Body: logFiles: One or more files to be uploaded(maxfiles: 10).
                -> Response
                    {
                        "message": "Logs uploaded successfully.",
                        "totalProcessed": 100,
                        "validLogs": 90,
                        "invalidLogs": 10
                    }

        b. Search Logs
            GET /search
                ->Description: Search logs by type, keyword, and date range.
                ->Query Parameters:
                      Key     |   significance   |                Description
                    type      |     optional     |     Filter logs by type (e.g., INFO, ERROR).
                    keyword   |     optional     |     Search for specific keywords in the logs.
                    startDate |     optional     |     Filter logs starting from this date.
                    endDate   |     optional     |     Filter logs up to this date.
                    sortBy    |     optional     |     Field to sort by (e.g., timestamp).
                    sortOrder |     optional     |     asc or desc.

                ->Response: 
                    {
                        "logs": [
                            {
                            "type": "INFO",
                            "timestamp": "2024-12-03T10:15:30Z",
                            "message": "Application started successfully."
                            }
                        ]
                    }

        c. Process Logs
            GET /process
                ->Description: Process unprocessed logs from the database and index them into Elasticsearch.
                Response: 
                    {
                        "message": "Logs processed successfully.",
                        "totalProcessed": 50,
                        "validLogs": 45,
                        "invalidLogs": 5
                    }  

System Design  
    1. Architecture
        ->Frontend: (future) Allows users to upload files and search logs.
        ->Backend: Built with Node.js and Express for handling file uploads, log validation, and querying.
        ->Database: 
            i. MongoDB: Stores raw logs and validation metadata.
            ii. Elasticsearch: Enables efficient log querying and filtering.
        ->File Storage: Uploaded files are temporarily stored in the uploads/ directory.

    2.Workflow
        ->File Upload:
            Accepts files via /upload API.
            Files are parsed, validated, and stored.
        ->Log Storage:
            Valid logs are indexed into Elasticsearch.
            Raw logs are stored in MongoDB.
        ->Log Query:
            Users can search logs using /search with filters and sorting.

Sample log formats supported
    1.JSON
        [
            {
                "timestamp": "2024-12-03T10:15:30Z",
                "type": "INFO",
                "message": "Application started successfully."
            }
        ]

    2. CSV
        timestamp,type,message
        2024-12-03T10:15:30Z,INFO,Application started successfully.
    
    3. Plain Text
        [2024-12-03T10:15:30Z] [INFO] Application started successfully.

Future Improvements

    1.Frontend Integration
        Designing the frontend using libraries and frameworks like react to upload and display search results along with performance analysis.
    2.Robust Queries
        Adding the advanced searching capabilities to the application like fuzzy matching search.
    3.Privacy and Security
        Securing the api through various methods of authentication and authorization.
    4.Cloud Integration
        Using a cloud based database that can store and process large amounts of data more efficiently.
    5.Robust Log Format analysis
        Can analyse various other log formats such as XML, apache etc.

--------------------------------------------------------------------------------------------------------------------------------------------------  
